{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600841913438",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Fasttext Data Pre-Processing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read and pre-process data\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer # word lemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords # stopwords\n",
    "\n",
    "# format data\n",
    "from sklearn.model_selection import train_test_split # train test split\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing(text, lemmatize=True, stem=False):\n",
    "    '''\n",
    "    Accepts a text and processes text\n",
    "    '''\n",
    "    # strip accents\n",
    "    text = text.encode('ascii', 'ignore')\n",
    "    text = str(text.decode(\"utf-8\"))\n",
    "\n",
    "    # covert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # remove punctuation\n",
    "    text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "\n",
    "    # remove unnecessary white spaces\n",
    "    text = text.replace(\"\\n\", \"\")\n",
    "\n",
    "    # tokenize\n",
    "    text_words = nltk.word_tokenize(text)\n",
    "\n",
    "    # lemmatize\n",
    "    if lemmatize:\n",
    "        wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        text_words = [wordnet_lemmatizer.lemmatize(x, pos=\"v\") for x in text_words]\n",
    "\n",
    "    # stem\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        text_words = [stemmer.stem(x) for x in text_words]\n",
    "\n",
    "    # remove stop words\n",
    "    stop = list(stopwords.words('english'))\n",
    "    keep_stopwords = [\"no\", \"not\", \"nor\"]\n",
    "    for word in keep_stopwords:\n",
    "        stop.remove(word)\n",
    "        stop = set(stop)\n",
    "    text_words = [x for x in text_words if not x in stop]\n",
    "\n",
    "    return ' '.join(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_data(texts, labels):\n",
    "    '''\n",
    "    Accepts a series of texts and labels and outputs the formatted data for fasttext model\n",
    "    '''\n",
    "    formatted_data = []\n",
    "\n",
    "    for i in range(len(texts)):\n",
    "        current_row = []\n",
    "\n",
    "        # prepare label\n",
    "        current_row.append(\"__label__\" + str(list(labels)[i]))\n",
    "\n",
    "        # prepare text\n",
    "        current_row.extend(nltk.word_tokenize(list(texts)[i]))\n",
    "\n",
    "        # add to output\n",
    "        formatted_data.append(current_row)\n",
    "    \n",
    "    return pd.Series(formatted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_train_data(text, label, filename):\n",
    "    '''\n",
    "    Accepts a list of texts, labels and filenames and saves the data into .txt file for each corresponding text, label and filename\n",
    "    '''\n",
    "    formatted_data = format_data(text, label)\n",
    "    # save data\n",
    "    with open(filename, \"w\") as csvoutfile:\n",
    "        csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "        for row in formatted_data:\n",
    "            csv_writer.writerow(row)\n",
    "\n",
    "def save_test_data(text, label, filename):\n",
    "    '''\n",
    "    Accepts a list of texts, labels and filenames and saves the data into a .csv file for reading later\n",
    "    '''\n",
    "    save_df = pd.concat([text, label], axis=1)\n",
    "    save_df = save_df.reset_index(drop=True) # reset index\n",
    "    save_df.columns = [\"text\", \"label\"] # standardise columns\n",
    "    save_df.to_csv(filename, index=False) # save to csv"
   ]
  },
  {
   "source": [
    "train_filenames = ['all_train', 'news_train', 'reddit_train', 'twitter_train']\n",
    "test_filenames = ['all_test', 'news_test', 'reddit_test', 'twitter_test']\n",
    "\n",
    "train_datasets = ['data/' + x + '.csv' for x in train_filenames]\n",
    "test_datasets = ['data/' + x + '.csv' for x in test_filenames]\n",
    "\n",
    "save_train_filenames = ['data/fasttext/normal/' + x + '.txt' for x in train_filenames]\n",
    "save_train_filenames_lemmatize = ['data/fasttext/lemmatize/' + x + '_lemmatize.txt' for x in train_filenames]\n",
    "save_train_filenames_stem = ['data/fasttext/stem/' + x + '_stem.txt' for x in train_filenames]\n",
    "\n",
    "save_test_filenames = ['data/fasttext/normal/' + x + '.csv' for x in test_filenames]\n",
    "save_test_filenames_lemmatize = ['data/fasttext/lemmatize/' + x + '_lemmatize.csv' for x in test_filenames]\n",
    "save_test_filenames_stem = ['data/fasttext/stem/' + x + '_stem.csv' for x in test_filenames]"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate train datasets\n",
    "for i in range(len(train_filenames)):\n",
    "    # load data\n",
    "    train_df = pd.read_csv(train_datasets[i], header=0)\n",
    "\n",
    "    # process text\n",
    "    train_df_lemmatize, train_df_stem = train_df.copy(deep=True), train_df.copy(deep=True)\n",
    "    train_df_lemmatize[\"text\"] = train_df_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "    train_df_stem[\"text\"] = train_df_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "    train_df[\"text\"] = train_df[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=False))\n",
    "\n",
    "    # save data\n",
    "    save_train_data(train_df[\"text\"], train_df[\"label\"], save_train_filenames[i])\n",
    "    save_train_data(train_df_lemmatize[\"text\"], train_df_lemmatize[\"label\"], save_train_filenames_lemmatize[i])\n",
    "    save_train_data(train_df_stem[\"text\"], train_df_stem[\"label\"], save_train_filenames_stem[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate test datasets\n",
    "for i in range(len(test_filenames)):\n",
    "    # load data\n",
    "    test_df = pd.read_csv(test_datasets[i], header=0)\n",
    "\n",
    "    # process text\n",
    "    test_df_lemmatize, test_df_stem = test_df.copy(deep=True), test_df.copy(deep=True)\n",
    "    test_df_lemmatize[\"text\"] = test_df_lemmatize[\"text\"].apply(lambda x: pre_processing(x, lemmatize=True, stem=False))\n",
    "    test_df_stem[\"text\"] = test_df_stem[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=True))\n",
    "    test_df[\"text\"] = test_df[\"text\"].apply(lambda x: pre_processing(x, lemmatize=False, stem=False))\n",
    "\n",
    "    # save data\n",
    "    save_test_data(test_df[\"text\"], test_df[\"label\"], save_test_filenames[i])\n",
    "    save_test_data(test_df_lemmatize[\"text\"], test_df_lemmatize[\"label\"], save_test_filenames_lemmatize[i])\n",
    "    save_test_data(test_df_stem[\"text\"], test_df_stem[\"label\"], save_test_filenames_stem[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}